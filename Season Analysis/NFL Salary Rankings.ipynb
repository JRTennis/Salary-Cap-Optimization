{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07c4f39f-ce23-4b27-944d-f97ff13d7197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping CapHit data for the year 2011...\n",
      "Delaying for 6.23 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2011...\n",
      "Delaying for 6.34 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2011...\n",
      "Delaying for 9.84 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2012...\n",
      "Delaying for 7.45 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2012...\n",
      "Delaying for 11.19 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2012...\n",
      "Delaying for 5.57 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2013...\n",
      "Delaying for 8.54 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2013...\n",
      "Delaying for 14.69 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2013...\n",
      "Delaying for 8.18 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2014...\n",
      "Delaying for 13.20 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2014...\n",
      "Delaying for 14.26 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2014...\n",
      "Delaying for 10.74 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2015...\n",
      "Delaying for 8.44 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2015...\n",
      "Delaying for 14.56 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2015...\n",
      "Delaying for 6.92 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2016...\n",
      "Delaying for 6.09 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2016...\n",
      "Delaying for 7.57 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2016...\n",
      "Delaying for 5.03 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2017...\n",
      "Delaying for 9.12 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2017...\n",
      "Delaying for 12.02 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2017...\n",
      "Delaying for 14.19 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2018...\n",
      "Delaying for 13.23 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2018...\n",
      "Delaying for 5.35 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2018...\n",
      "Delaying for 9.77 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2019...\n",
      "Delaying for 14.53 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2019...\n",
      "Delaying for 8.39 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2019...\n",
      "Delaying for 5.52 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2020...\n",
      "Delaying for 7.83 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2020...\n",
      "Delaying for 12.97 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2020...\n",
      "Delaying for 11.19 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2021...\n",
      "Delaying for 6.09 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2021...\n",
      "Delaying for 13.59 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2021...\n",
      "Delaying for 11.87 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2022...\n",
      "Delaying for 9.75 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2022...\n",
      "Delaying for 9.23 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2022...\n",
      "Delaying for 14.05 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2023...\n",
      "Delaying for 9.37 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2023...\n",
      "Delaying for 12.79 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2023...\n",
      "Delaying for 11.19 seconds to avoid detection...\n",
      "Scraping CapHit data for the year 2024...\n",
      "Delaying for 14.29 seconds to avoid detection...\n",
      "Scraping ContractAverage data for the year 2024...\n",
      "Delaying for 9.04 seconds to avoid detection...\n",
      "Scraping ContractLength data for the year 2024...\n",
      "Delaying for 5.55 seconds to avoid detection...\n",
      "Data has been scraped and saved to NFL_Salary_Rankings_2011_2024.xlsx with proper formatting.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Base URLs for each category\n",
    "base_urls = {\n",
    "    \"CapHit\": \"https://www.spotrac.com/nfl/rankings/player/_/year/{}/sort/cap_total\",\n",
    "    \"ContractAverage\": \"https://www.spotrac.com/nfl/rankings/player/_/year/{}/sort/contract_average\",\n",
    "    \"ContractLength\": \"https://www.spotrac.com/nfl/rankings/player/_/year/{}/sort/contract_length\"\n",
    "}\n",
    "\n",
    "# List of years to scrape\n",
    "years = range(2011, 2025)\n",
    "\n",
    "# User-Agent pool for rotation\n",
    "user_agents = [\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/105.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/104.0.0.0 Safari/537.36\",\n",
    "    \"Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/103.0.5060.134 Safari/537.36\",\n",
    "]\n",
    "\n",
    "# Dictionary to store all scraped data\n",
    "all_data = {category: [] for category in base_urls.keys()}\n",
    "\n",
    "# Function to clean numeric values\n",
    "def clean_numeric(value, is_currency=False):\n",
    "    \"\"\"\n",
    "    Extracts the numeric portion of a string that starts with a dollar sign ($), \n",
    "    ignoring any text before the dollar sign.\n",
    "    \"\"\"\n",
    "    # Search for a dollar sign followed by the numeric value\n",
    "    match = re.search(r'\\$(\\d[\\d,]*(?:\\.\\d{1,2})?)', value)\n",
    "    if match:\n",
    "        # Extract the numeric portion\n",
    "        cleaned = match.group(1)\n",
    "        numeric_value = float(cleaned.replace(',', ''))\n",
    "        return int(numeric_value) if not is_currency else numeric_value\n",
    "    return None\n",
    "\n",
    "# Function to scrape a single category for a single year\n",
    "def scrape_category(year, category, url):\n",
    "    print(f\"Scraping {category} data for the year {year}...\")\n",
    "    headers = {\"User-Agent\": random.choice(user_agents)}\n",
    "    try:\n",
    "        response = requests.get(url, headers=headers)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        # Initialize lists for this category and year\n",
    "        ranks = []\n",
    "        players = []\n",
    "        teams = []\n",
    "        positions = []\n",
    "        values = []\n",
    "\n",
    "        # Locate the list-group containing the players\n",
    "        players_list = soup.find_all('li', class_='list-group-item')\n",
    "\n",
    "        # Extract data\n",
    "        for player in players_list:\n",
    "            try:\n",
    "                # Rank\n",
    "                rank = player.find('div', class_='fw-bold').text.strip()\n",
    "                \n",
    "                # Player Name\n",
    "                player_name = player.find('div', class_='link').text.strip()\n",
    "                \n",
    "                # Team and Position\n",
    "                team_pos = player.find('small').text.strip()\n",
    "                team, position = map(str.strip, team_pos.split(','))\n",
    "                \n",
    "                # Value (Cap Hit, Contract Average, or Contract Length)\n",
    "                raw_value = player.find('span', class_='medium').text.strip()\n",
    "                if category in ['CapHit', 'ContractAverage']:\n",
    "                    value = clean_numeric(raw_value, is_currency=True)  # Keep currency formatting\n",
    "                elif category == 'ContractLength':\n",
    "                    value = clean_numeric(raw_value, is_currency=False)  # Strip currency formatting\n",
    "                    \n",
    "                # Append to lists\n",
    "                ranks.append(rank)\n",
    "                players.append(player_name)\n",
    "                teams.append(team)\n",
    "                positions.append(position)\n",
    "                values.append(value)\n",
    "            except AttributeError:\n",
    "                # Skip problematic rows (e.g., banners)\n",
    "                continue\n",
    "\n",
    "        # Create a DataFrame for this category and year\n",
    "        df = pd.DataFrame({\n",
    "            'Season': [year] * len(ranks),\n",
    "            'Rank': ranks,\n",
    "            'Player': players,\n",
    "            'Team': teams,\n",
    "            'Position': positions,\n",
    "            category: values\n",
    "        })\n",
    "\n",
    "        # Convert Rank to numeric and handle ties\n",
    "        df['Rank'] = pd.to_numeric(df['Rank'], errors='coerce')\n",
    "        df['Rank'] = df['Rank'].ffill()\n",
    "\n",
    "        # Return the DataFrame\n",
    "        return df\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Failed to scrape {category} data for {year}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Loop through each year and category\n",
    "for year in years:\n",
    "    for category, base_url in base_urls.items():\n",
    "        # Construct the URL for this category and year\n",
    "        url = base_url.format(year)\n",
    "\n",
    "        # Scrape the data\n",
    "        df = scrape_category(year, category, url)\n",
    "        if df is not None:\n",
    "            all_data[category].append(df)\n",
    "\n",
    "        # Introduce a random delay between requests\n",
    "        delay = random.uniform(5, 15)  # Delay between 5 and 15 seconds\n",
    "        print(f\"Delaying for {delay:.2f} seconds to avoid detection...\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "# Combine all data for each category into a single DataFrame\n",
    "final_dataframes = {category: pd.concat(all_dfs, ignore_index=True) for category, all_dfs in all_data.items()}\n",
    "\n",
    "# Save all data to an Excel file with separate sheets\n",
    "output_file = \"NFL_Salary_Rankings_2011_2024.xlsx\"\n",
    "with pd.ExcelWriter(output_file, engine='xlsxwriter') as writer:\n",
    "    for category, df in final_dataframes.items():\n",
    "        # Write data to sheet\n",
    "        df.to_excel(writer, index=False, sheet_name=category)\n",
    "\n",
    "        # Apply formatting\n",
    "        workbook = writer.book\n",
    "        worksheet = writer.sheets[category]\n",
    "        if category in ['CapHit', 'ContractAverage']:\n",
    "            # Format currency columns\n",
    "            currency_format = workbook.add_format({'num_format': '$#,##0', 'align': 'left'})\n",
    "            worksheet.set_column(df.columns.get_loc(category), df.columns.get_loc(category), 15, currency_format)\n",
    "        elif category == 'ContractLength':\n",
    "            # Format number columns\n",
    "            number_format = workbook.add_format({'num_format': '0', 'align': 'left'})\n",
    "            worksheet.set_column(df.columns.get_loc(category), df.columns.get_loc(category), 15, number_format)\n",
    "\n",
    "print(f\"Data has been scraped and saved to {output_file} with proper formatting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8900eced-7376-40f8-8023-4f66ddb4439c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
